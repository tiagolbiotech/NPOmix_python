{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import networkx\n",
    "from networkx.algorithms.components.connected import connected_components\n",
    "from packages import npomix\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import jaccard_score\n",
    "import csv #new\n",
    "import matplotlib.pyplot as plt #new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GNPS files\n",
    "    #networkedges_selfloop\n",
    "edges_path = \"/home/andres/Documents/Lab_Actino/UCSD_work/1000_Genomes/NPOmix/NPOmix_data/GNPS_321/networkedges_selfloop/ca909324641045abbfa26b6d21c74c1d..selfloop\"\n",
    "    #clusterinfosummarygroup file\n",
    "nodes_path = \"/home/andres/Documents/Lab_Actino/UCSD_work/1000_Genomes/NPOmix/NPOmix_data/GNPS_321/clusterinfosummarygroup_attributes_withIDs_withcomponentID/d659f4f36de74a7ebdefe626d58024d3.clustersummary\"\n",
    "    #result_specnets_DB\n",
    "hits_path = pd.read_csv(\"/home/andres/Documents/Lab_Actino/UCSD_work/1000_Genomes/NPOmix/NPOmix_data/GNPS_321/result_specnets_DB/b527f46471ad41cca3a503816f82d3ef.tsv\",sep='\\t')\n",
    "\n",
    "#BGCs files\n",
    "    #BiG-SCAPE network\n",
    "input_bigscape_net = \"/home/andres/Documents/Lab_Actino/UCSD_work/1000_Genomes/NPOmix/NPOmix_data/Bigscape/output_bigscape_mibig31_filteredclusters321/bigscape_all_c030_321strains.txt\"\n",
    "    #BiG-SCAPE annotation table\n",
    "input_bigscape_ann = \"/home/andres/Documents/Lab_Actino/UCSD_work/1000_Genomes/NPOmix/NPOmix_data/Bigscape/output_bigscape_mibig31_filteredclusters321/Network_Annotations_Full.tsv\" #new\n",
    "    #antiSMASH results folder\n",
    "antismash_folder = \"/home/andres/Documents/Lab_Actino/UCSD_work/1000_Genomes/NPOmix/NPOmix_data/antismash_only_gbk/\"\n",
    "\n",
    "#NPOmix files\n",
    "ena_df_file = \"/home/andres/Documents/Lab_Actino/UCSD_work/1000_Genomes/NPOmix/NPOmix_python/ena_dict-210315.csv\"\n",
    "    #matched MIBiG BGCs with spec lib\n",
    "mibig_df = pd.read_csv(\"/home/andres/Documents/Lab_Actino/UCSD_work/1000_Genomes/NPOmix/NPOmix_python/matched_mibig_gnps_update.tsv\",sep='\\t')\n",
    "    #MIBiG 3.1 main product table\n",
    "mibig_prod = pd.read_csv(\"MIBiG_products.csv\",sep=',') #new\n",
    "\n",
    "#Specify results folder\n",
    "results_folder = '/home/andres/Documents/Lab_Actino/UCSD_work/1000_Genomes/NPOmix/NPOmix_data/20230127_notderep_321_edited'\n",
    "\n",
    "current_date = datetime.today().strftime('%Y%m%d')\n",
    "\n",
    "if not os.path.isdir(results_folder):\n",
    "    os.mkdir(results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df = pd.read_csv(nodes_path,sep='\\t')\n",
    "\n",
    "nodes_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterindex_list = []\n",
    "\n",
    "for i,r in nodes_df.iterrows():\n",
    "# remove this line to use everything\n",
    "#    if type(r['LibraryID']) == float:\n",
    "        clusterindex_list.append(r['cluster index'])\n",
    "        \n",
    "clusterindex_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = pd.read_csv(edges_path,sep='\\t')\n",
    "\n",
    "def get_neighbors(target,dataframe,column1,column2):\n",
    "    subset1 = dataframe[(dataframe[column1]==target)]\n",
    "    subcat = subset1.append(dataframe[(dataframe[column2]==target)])\n",
    "    temp_list = []\n",
    "    for index,row in subcat.iterrows():\n",
    "        temp_list.append(subcat[column1][index])\n",
    "        temp_list.append(subcat[column2][index])\n",
    "    temp_list = list(np.unique(temp_list))\n",
    "    return temp_list\n",
    "\n",
    "def to_edges(l):\n",
    "    it = iter(l)\n",
    "    last = next(it)\n",
    "    for current in it:\n",
    "        yield last, current\n",
    "        last = current\n",
    "\n",
    "def to_graph(l):\n",
    "    G = networkx.Graph()\n",
    "    for part in l:\n",
    "        G.add_nodes_from(part)\n",
    "        G.add_edges_from(to_edges(part))\n",
    "    return G\n",
    "\n",
    "def get_family_dict(components_list,dataframe,dictionary,column1,column2,column3):\n",
    "    count = 0\n",
    "    for family in list(components_list):\n",
    "        count += 1\n",
    "        for fam_member in family:\n",
    "            dictionary['MF%s'%count].append(fam_member)\n",
    "    return dictionary\n",
    "\n",
    "def main_get_families(gnps_df):\n",
    "    targets_list = np.unique([gnps_df.CLUSTERID1,gnps_df.CLUSTERID2])\n",
    "    neighbors_list = []\n",
    "    for target in targets_list:\n",
    "        neighbors_list.append(get_neighbors(target,gnps_df,'CLUSTERID1','CLUSTERID2'))\n",
    "    G = to_graph(neighbors_list)\n",
    "    C = connected_components(G)\n",
    "    mf_dict = defaultdict(list)\n",
    "    mf_dict = get_family_dict(C,gnps_df,mf_dict,'CLUSTERID1','CLUSTERID2','Cosine')\n",
    "    return mf_dict\n",
    "\n",
    "mf_dict = main_get_families(edges_df)\n",
    "\n",
    "mf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ion in clusterindex_list:\n",
    "    print(ion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a table with MFs and cluster indexes\n",
    "mf_df = pd.DataFrame.from_dict(mf_dict, orient='index') #convert from dict list to df\n",
    "mf_dfwide = pd.melt(mf_df, ignore_index=False, id_vars=None) #melt to long format\n",
    "#mf_dfwide = mf_dfwide.drop(['data','index','index_names']) #remove rows with data, index and index_names\n",
    "mf_dfwide = mf_dfwide.drop(columns=['variable']) #remove columns\n",
    "mf_dfwide = mf_dfwide.dropna() #remove rows with NaN\n",
    "mf_dfwide = mf_dfwide.sort_values(by=['value']) #sort by cluster index\n",
    "mf_dfwide.reset_index(inplace=True) #set index as column\n",
    "mf_dfwide = mf_dfwide.rename(columns={'index':'MFs','value':'cluster index'}) #rename columns\n",
    "print(mf_dfwide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df2= nodes_df[[\"cluster index\",\"DefaultGroups\",\"UniqueFileSources\",\"parent mass\",\"sum(precursor intensity)\",\"LibraryID\"]]\n",
    "\n",
    "#Concatenate Node tables with MFs\n",
    "nodes_mf = pd.merge(nodes_df2, mf_dfwide, right_index = True, left_index = True)\n",
    "nodes_mf  = nodes_mf.drop(columns=['cluster index_y']) #remove columns\n",
    "nodes_mf  = nodes_mf.rename(columns={'cluster index_x':'metabolite_ID','UniqueFileSources':'mzML_file_sources','sum(precursor intensity)':'sum_intensity','parent mass':'parent_mass'}) #rename columns\n",
    "nodes_mf = nodes_mf[[\"metabolite_ID\",\"MFs\",\"DefaultGroups\",\"mzML_file_sources\",\"parent_mass\",\"LibraryID\"]]\n",
    "#include GNPS compound classes\n",
    "hits_path2 = hits_path[[\"#Scan#\",\"npclassifier_superclass\",\"npclassifier_class\",\"npclassifier_pathway\"]] #select only scan# and npclassifier columns\n",
    "hits_path2  = hits_path2.rename(columns={'#Scan#':'metabolite_ID'}) #rename columns\n",
    "nodes_mf = pd.merge(nodes_mf, hits_path2, how = 'left' ,on='metabolite_ID')\n",
    "nodes_mf.to_csv(os.path.join(results_folder,'NodesMF_dic-NPOmix1.0-%s.txt'%(current_date)))\n",
    "nodes_mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcms_file_list = []\n",
    "\n",
    "for item in nodes_df['UniqueFileSources']:\n",
    "    for lcms_file in item.split('|'):\n",
    "        if lcms_file not in lcms_file_list:\n",
    "            print(lcms_file)\n",
    "            lcms_file_list.append(lcms_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lcms_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_presence_files(nodes_df):\n",
    "    for unique_list in list(nodes_df[nodes_df['cluster index'] == ion]['UniqueFileSources']):\n",
    "        return list(unique_list.split('|'))\n",
    "\n",
    "all_rows_list,testing_indexes_list = [],[]\n",
    "for ion in clusterindex_list:\n",
    "    subset_edges = edges_df[(edges_df.CLUSTERID1 == ion) | (edges_df.CLUSTERID2 == ion)]\n",
    "    cosine = round(max(subset_edges['Cosine']),2)\n",
    "    presence_list = get_presence_files(nodes_df)\n",
    "    single_row_list = []\n",
    "    for lcms_file in lcms_file_list:\n",
    "        if lcms_file in presence_list:\n",
    "            single_row_list.append(cosine)\n",
    "        else:\n",
    "            single_row_list.append(0)\n",
    "    all_rows_list.append(single_row_list)\n",
    "    testing_indexes_list.append(ion)\n",
    "    \n",
    "all_rows_list\n",
    "\n",
    "pre_testing_df = pd.DataFrame(all_rows_list,index=testing_indexes_list,columns=lcms_file_list)\n",
    "\n",
    "pre_testing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_df(training_df,testing_df,neighbors_array,results_folder):\n",
    "    final_df = pd.DataFrame(columns=('metabolite_ID','predicted_GCFs','max_jaccard','jaccard_scores','parent_mass','sum_intensity')) #added jaccard_scores\n",
    "    for i,ccms_id in enumerate(testing_df.index):\n",
    "        jaccard_scores = []\n",
    "        for j in range(0,len(neighbors_array[i])):\n",
    "            query_bgc = neighbors_array[i][j]\n",
    "            bgc_fp = training_df[training_df['label'] == query_bgc].iloc[0]\n",
    "            bgc_fp = bgc_fp.drop(\"label\")\n",
    "            ms_fp = testing_df.loc[ccms_id]\n",
    "            bgc_binary = npomix.get_binary(bgc_fp)\n",
    "            ms_binary = npomix.get_binary(ms_fp)\n",
    "            jaccard_scores.append(jaccard_score(bgc_binary,ms_binary))\n",
    "        max_jaccard = round(max(jaccard_scores),2)\n",
    "        parentmass = nodes_df[nodes_df['cluster index'] == ccms_id]['precursor mass'].item()\n",
    "        intensity = nodes_df[nodes_df['cluster index'] == ccms_id]['sum(precursor intensity)'].item()\n",
    "        final_df.loc[i] = ccms_id,neighbors_array[i],max_jaccard,jaccard_scores,parentmass,intensity #added jaccard_scores\n",
    "    #final_df.to_csv(\"%sfinal_df-noderep-NPOmix1.0-%s.txt\"%(results_folder,current_date),sep=\"\\t\",index_label=False)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_value = 3\n",
    "\n",
    "merged_ispec_mat = npomix.get_merged_ispec_mat(pre_testing_df)\n",
    "merged_ispec_mat = npomix.renaming_merged_ispec_mat(ena_df_file,merged_ispec_mat)\n",
    "print('Obtaining BiG-SCAPE dataframe and BiG-SCAPE dictionary')\n",
    "bigscape_df,bigscape_dict = npomix.get_bigscape_df(ena_df_file,input_bigscape_net)\n",
    "bigscape_df,bigscape_dict2 = npomix.rename_bigscape_df(antismash_folder,bigscape_df,bigscape_dict)\n",
    "print('BiG-SCAPE create with %s GCFs'%len(bigscape_dict))\n",
    "strain_list,bgcs_list = npomix.get_strain_list(bigscape_df)\n",
    "print('Creating training dataframe')\n",
    "affinity_df,affinity_bgcs = npomix.get_pre_training_df(bigscape_df,bigscape_dict2,strain_list,bgcs_list)\n",
    "affinity_df = npomix.renaming_affinity_df(affinity_df)\n",
    "networked_cols = npomix.get_networked_cols(merged_ispec_mat,affinity_df)\n",
    "training_df,training_bgcs = npomix.get_training_df(affinity_df,networked_cols,results_folder,affinity_bgcs)\n",
    "bgcs_df = pd.DataFrame(training_bgcs, columns=['bgcs'])\n",
    "testing_df = npomix.get_testing_df(merged_ispec_mat,networked_cols,results_folder)\n",
    "print('Running KNN using k equals to %s'%k_value)\n",
    "neighbors_array = npomix.running_knn(training_df,testing_df,k_value)\n",
    "print('Creating final dataframe')\n",
    "final_df = get_final_df(training_df,testing_df,neighbors_array,results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[final_df['max_jaccard'] > 0]\n",
    "final_df = final_df.sort_values(by='max_jaccard',ascending=False)\n",
    "final_df = final_df.reset_index(drop=True)\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListFiles(dirName):\n",
    "    listFile = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    for entry in listFile:\n",
    "        fullPath = os.path.join(dirName, entry)\n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles = allFiles + getListFiles(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "                \n",
    "    return allFiles\n",
    "\n",
    "BGC_list= getListFiles(antismash_folder)\n",
    "\n",
    "#convert from BGC list to df\n",
    "BGC_df = pd.DataFrame(BGC_list,columns=['BGCs'])\n",
    "BGC_df = BGC_df.BGCs.str.split(\"/\", expand = True) #separate all the folders in columns\n",
    "BGC_df = BGC_df.iloc[:,-2:] #select only the last two columns, strain folder and the BGCs present in that folder\n",
    "BGC_df = BGC_df.rename(columns={10:'strain', 11:'BGC'}) #rename columns Needs to be improved so it changes for any column name\n",
    "BGC_df = BGC_df[[\"BGC\",\"strain\"]]\n",
    "\n",
    "#Get a table with the folder containing BGCs\n",
    "strain_df = BGC_df.strain.str.split(\".\", expand = True) #separate folder name by a point\n",
    "strain_df = strain_df.rename(columns={0:'strain'}) #rename columns\n",
    "strain_df = strain_df[['strain']]\n",
    "\n",
    "#Merge table with BGCs and respective strains\n",
    "BGC_df = BGC_df[[\"BGC\"]]\n",
    "BGC_df = pd.merge(BGC_df, strain_df, right_index = True, left_index = True)\n",
    "BGC_df = BGC_df.replace('.gbk','',regex=True)\n",
    "BGC_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a table with GCFss and BGCs\n",
    "GCF_df = pd.DataFrame.from_dict(bigscape_dict, orient='index') #convert from dict list to df\n",
    "GCF_dfwide = pd.melt(GCF_df, ignore_index=False, id_vars=None) #melt to long format\n",
    "GCF_dfwide = GCF_dfwide.drop(columns=['variable']) #remove columns\n",
    "GCF_dfwide = GCF_dfwide.dropna() #remove rows with NaN\n",
    "GCF_dfwide = GCF_dfwide.sort_values(by=['value']) #sort by cluster index\n",
    "GCF_dfwide.reset_index(inplace=True) #set index as column\n",
    "GCF_dfwide = GCF_dfwide.rename(columns={'index':'GCFs','value':'BGC'}) #rename columns\n",
    "GCF_dfwide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate GCFs with BiG-SCAPE annotation table\n",
    "ann_df = pd.read_table(input_bigscape_ann, delimiter='\\t')\n",
    "ann_df = ann_df.rename(columns={'BiG-SCAPE class':'BiG-SCAPE_class'})\n",
    "ann_df = ann_df[[\"BGC\",\"BiG-SCAPE_class\"]]\n",
    "ann_df = pd.merge(GCF_dfwide, ann_df, how = 'right' ,on='BGC')\n",
    "ann_df = pd.merge(ann_df, mibig_prod, how = 'left' ,on='BGC')\n",
    "ann_df = ann_df[[\"BGC\",\"GCFs\",\"MIBiG_main_product\",\"BiG-SCAPE_class\"]]\n",
    "ann_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge with table with BGCs and strain info\n",
    "GCF_BGC = pd.merge(ann_df, BGC_df, how = 'outer' ,on='BGC')\n",
    "GCF_BGC.GCFs.fillna('singleton', inplace=True)\n",
    "GCF_BGC.to_csv(os.path.join(results_folder,'Bigscape_dic-NPOmix1.0-%s.txt'%(current_date)))\n",
    "GCF_BGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mibig_name_dict = dict(zip(mibig_df['mibig_id'],mibig_df['mibig_name']))\n",
    "\n",
    "ccmsid_mibig_dict = dict(zip(mibig_df['# mgf_spectrum_id'],mibig_df['mibig_id']))\n",
    "\n",
    "ccmsid_mibig_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_final_df = final_df[final_df['max_jaccard'] >= 0.7]\n",
    "\n",
    "#Concatenate Node tables with predicted_GCFs\n",
    "mf_gcf = pd.merge(nodes_mf, subset_final_df, left_on='metabolite_ID', right_on='metabolite_ID')\n",
    "mf_gcf  = mf_gcf.drop(columns=['parent_mass_y']) #remove columns\n",
    "mf_gcf  = mf_gcf.rename(columns={'parent_mass_x':'parent_mass'}) #rename columns\n",
    "mf_gcf.to_csv(os.path.join(results_folder,'mf_gcf_all_dic-NPOmix1.0-%s.txt'%(current_date)))\n",
    "\n",
    "#Define a table without the training dataset (G1)\n",
    "mf_gcf_g2 = mf_gcf\n",
    "mf_gcf_g2 = mf_gcf_g2[mf_gcf_g2.DefaultGroups != \"G1\"]\n",
    "mf_gcf_g2.to_csv(os.path.join(results_folder,'mf_gcf_g2_dic-NPOmix1.0-%s.txt'%(current_date)))\n",
    "\n",
    "mf_gcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a table with MFs counts\n",
    "MFs_count = nodes_mf['MFs'].value_counts()\n",
    "MFs_count = pd.DataFrame.from_dict(MFs_count) #convert from dict list to df\n",
    "MFs_count.reset_index(inplace=True) #set index as column\n",
    "MFs_count = MFs_count.rename(columns={'MFs':'count','index':'MFs'}) #rename columns\n",
    "\n",
    "#Define a list of MFs\n",
    "MFs_list = []\n",
    "for i,r in nodes_mf.iterrows():\n",
    "    if type(r['LibraryID']) != float:\n",
    "        MFs_list.append(r['MFs'])\n",
    "#Define a list of Library ID\n",
    "Lib_list = []        \n",
    "for i,r in nodes_mf.iterrows():\n",
    "    if type(r['LibraryID']) != float:        \n",
    "        Lib_list.append(r['LibraryID'])\n",
    "\n",
    "#Convert to dataframe and merge MFs and LibraryIDs        \n",
    "MFs_list = pd.DataFrame.from_dict(MFs_list) #convert from dict list to df \n",
    "MFs_list = MFs_list.rename(columns={0:'MFs'})\n",
    "Lib_list = pd.DataFrame.from_dict(Lib_list) #convert from dict list to df \n",
    "Lib_list = Lib_list.rename(columns={0:'LibraryIDs'})\n",
    "MFs_list = pd.concat([MFs_list, Lib_list], axis=1).drop_duplicates() #concatenate and drop duplicates\n",
    "\n",
    "#Group MFs with more than one annotation using a ;\n",
    "MFs_list2 = MFs_list.groupby(['MFs'])['LibraryIDs'].apply('; '.join).reset_index()\n",
    "\n",
    "#Merge with MFs counts\n",
    "MFs_list2 = pd.merge(MFs_count, MFs_list2, how='left', on='MFs')\n",
    "\n",
    "#Create a table with information about MFs and default GNPS groups. G1 training dataset, G2 other dataset\n",
    "Groups = nodes_mf[[\"MFs\",\"DefaultGroups\"]].drop_duplicates()\n",
    "Groups = Groups.groupby(['MFs'])['DefaultGroups'].apply('; '.join).reset_index()\n",
    "\n",
    "#Merge with MFs counts\n",
    "MFs_list2 = pd.merge(MFs_list2, Groups, how='left', on='MFs')\n",
    "#Replace everything other than G1 or G2 with G1,G2\n",
    "MFs_list2['DefaultGroups'] = np.where(MFs_list2['DefaultGroups'].isin(['G1','G2']), MFs_list2['DefaultGroups'], 'G1,G2')\n",
    "#Add a column that indicates that these are MF\n",
    "MFs_list2.insert(4,'Type','MF')\n",
    "MFs_list2 = MFs_list2.rename(columns={'LibraryIDs':'Annotation'})\n",
    "\n",
    "MFs_list2.to_csv(os.path.join(results_folder,'Network_metadata_MFs_list_all-NPOmix1.0-%s.txt'%(current_date)))\n",
    "MFs_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a table with GCFs counts\n",
    "GCFs_count = GCF_BGC['GCFs'].value_counts()\n",
    "GCFs_count = pd.DataFrame.from_dict(GCFs_count) #convert from dict list to df\n",
    "GCFs_count.reset_index(inplace=True) #set index as column\n",
    "GCFs_count = GCFs_count.rename(columns={'GCFs':'count','index':'GCFs'}) #rename columns\n",
    "GCFs_count = GCFs_count[GCFs_count['GCFs'] != 'singleton']\n",
    "#Define a list of GCFs with MIBiG hits\n",
    "GCFs_list = []\n",
    "for i,r in GCF_BGC.iterrows():\n",
    "    if type(r['MIBiG_main_product']) != float:\n",
    "        GCFs_list.append(r['GCFs'])\n",
    "#Define a list of MIBiG hits\n",
    "MIBiG_list = []        \n",
    "for i,r in GCF_BGC.iterrows():\n",
    "    if type(r['MIBiG_main_product']) != float:        \n",
    "        MIBiG_list.append(r['MIBiG_main_product'])\n",
    "        \n",
    "#Convert to dataframe and merge GCFs and MIBiG main products        \n",
    "GCFs_list = pd.DataFrame.from_dict(GCFs_list) #convert from dict list to df \n",
    "GCFs_list = GCFs_list.rename(columns={0:'GCFs'})\n",
    "MIBiG_list = pd.DataFrame.from_dict(MIBiG_list) #convert from dict list to df \n",
    "MIBiG_list = MIBiG_list.rename(columns={0:'Annotation'})\n",
    "GCFs_list = pd.concat([GCFs_list, MIBiG_list], axis=1).drop_duplicates() #concatenate and drop duplicates       \n",
    "GCFs_list = GCFs_list[GCFs_list['GCFs'] != 'singleton']\n",
    "\n",
    "#Group GCFs with more than one annotation using a ;\n",
    "GCFs_list = GCFs_list.groupby(['GCFs'])['Annotation'].apply('; '.join).reset_index()\n",
    "#Merge with MFs counts\n",
    "GCFs_list = pd.merge(GCFs_count, GCFs_list, how='left', on='GCFs')\n",
    "GCFs_list.insert(3,'Type','GCF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counts by BGCs classes\n",
    "#NRPS\n",
    "NRPS_BGC = GCF_BGC[GCF_BGC['BiG-SCAPE_class'] == 'NRPS'] #filter only NRPSs from table\n",
    "NRPS_count = NRPS_BGC['GCFs'].value_counts().to_frame() #count\n",
    "NRPS_count.reset_index(inplace=True) #set index as column\n",
    "NRPS_count = NRPS_count.rename(columns={'GCFs':'NRPS','index':'GCFs'}) #rename columns\n",
    "NRPS_count = NRPS_count[NRPS_count['GCFs'] != 'singleton'] #remove singletons\n",
    "GCFs_list = pd.merge(GCFs_list,NRPS_count, how='left', on='GCFs')\n",
    "\n",
    "#others\n",
    "Others_BGC = GCF_BGC[GCF_BGC['BiG-SCAPE_class'] == 'Others'] #filter only NRPSs from table\n",
    "Others_count = Others_BGC['GCFs'].value_counts().to_frame() #count\n",
    "Others_count.reset_index(inplace=True) #set index as column\n",
    "Others_count = Others_count.rename(columns={'GCFs':'Others','index':'GCFs'}) #rename columns\n",
    "Others_count = Others_count[Others_count['GCFs'] != 'singleton'] #remove singletons\n",
    "GCFs_list = pd.merge(GCFs_list,Others_count, how='left', on='GCFs')\n",
    "\n",
    "#PKS-NRP_Hybrids\n",
    "PKSNRP_Hybrids_BGC = GCF_BGC[GCF_BGC['BiG-SCAPE_class'] == 'PKS-NRP_Hybrids'] #filter only NRPSs from table\n",
    "PKSNRP_Hybrids_count = PKSNRP_Hybrids_BGC['GCFs'].value_counts().to_frame() #count\n",
    "PKSNRP_Hybrids_count.reset_index(inplace=True) #set index as column\n",
    "PKSNRP_Hybrids_count = PKSNRP_Hybrids_count.rename(columns={'GCFs':'PKS_NRP_Hybrids','index':'GCFs'}) #rename columns\n",
    "PKSNRP_Hybrids_count = PKSNRP_Hybrids_count[PKSNRP_Hybrids_count['GCFs'] != 'singleton'] #remove singletons\n",
    "GCFs_list = pd.merge(GCFs_list,PKSNRP_Hybrids_count, how='left', on='GCFs')\n",
    "\n",
    "#PKSI\n",
    "PKSI_BGC = GCF_BGC[GCF_BGC['BiG-SCAPE_class'] == 'PKSI'] #filter only NRPSs from table\n",
    "PKSI_count = PKSI_BGC['GCFs'].value_counts().to_frame() #count\n",
    "PKSI_count.reset_index(inplace=True) #set index as column\n",
    "PKSI_count = PKSI_count.rename(columns={'GCFs':'PKSI','index':'GCFs'}) #rename columns\n",
    "PKSI_count = PKSI_count[PKSI_count['GCFs'] != 'singleton'] #remove singletons\n",
    "GCFs_list = pd.merge(GCFs_list,PKSI_count, how='left', on='GCFs')\n",
    "\n",
    "#PKSother\n",
    "PKSother_BGC = GCF_BGC[GCF_BGC['BiG-SCAPE_class'] == 'PKSother'] #filter only NRPSs from table\n",
    "PKSother_count = PKSother_BGC['GCFs'].value_counts().to_frame() #count\n",
    "PKSother_count.reset_index(inplace=True) #set index as column\n",
    "PKSother_count = PKSother_count.rename(columns={'GCFs':'PKSother','index':'GCFs'}) #rename columns\n",
    "PKSother_count = PKSother_count[PKSother_count['GCFs'] != 'singleton'] #remove singletons\n",
    "GCFs_list = pd.merge(GCFs_list,PKSother_count, how='left', on='GCFs')\n",
    "\n",
    "#RiPPs\n",
    "RiPPs_BGC = GCF_BGC[GCF_BGC['BiG-SCAPE_class'] == 'RiPPs']\n",
    "RiPPs_count = RiPPs_BGC['GCFs'].value_counts().to_frame()\n",
    "RiPPs_count.reset_index(inplace=True) #set index as column\n",
    "RiPPs_count = RiPPs_count.rename(columns={'GCFs':'RiPPs','index':'GCFs'}) #rename columns\n",
    "RiPPs_count = RiPPs_count[RiPPs_count['GCFs'] != 'singleton']\n",
    "GCFs_list = pd.merge(GCFs_list,RiPPs_count, how='left', on='GCFs')\n",
    "\n",
    "#Terpene\n",
    "Terpene_BGC = GCF_BGC[GCF_BGC['BiG-SCAPE_class'] == 'Terpene']\n",
    "Terpene_count = Terpene_BGC['GCFs'].value_counts().to_frame()\n",
    "Terpene_count.reset_index(inplace=True) #set index as column\n",
    "Terpene_count = Terpene_count.rename(columns={'GCFs':'Terpene','index':'GCFs'}) #rename columns\n",
    "Terpene_count = Terpene_count[Terpene_count['GCFs'] != 'singleton']\n",
    "GCFs_list = pd.merge(GCFs_list,Terpene_count, how='left', on='GCFs')\n",
    "GCFs_list = GCFs_list.rename(columns={'MIBiG_main_product':'Annotation'})\n",
    "\n",
    "GCFs_list.to_csv(os.path.join(results_folder,'Network_metadata_GCFs_list-NPOmix1.0-%s.txt'%(current_date)))\n",
    "GCFs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a network that connects MFs with the three GCFs\n",
    "ntwks = mf_gcf[[\"MFs\",\"predicted_GCFs\",\"jaccard_scores\"]]\n",
    "\n",
    "#Loop to make a df with each GCF according to k_value\n",
    "for i in range(1,k_value+1):\n",
    "    exec(f'ntwks{i}=mf_gcf[[\"predicted_GCFs\"]]')\n",
    "    exec(f'ntwks{i}[\"predicted_GCFs\"] = ntwks{i}[\"predicted_GCFs\"].str.get({i-1})')\n",
    "\n",
    "nt_GCF=pd.concat([ntwks1, ntwks2, ntwks3], axis=1)\n",
    "nt_GCF.columns=[\"GCF\"+str(i) for i in range(1,k_value+1)]\n",
    "\n",
    "#Loop to make a df with each jaccard according to k_value\n",
    "for i in range(1,k_value+1):\n",
    "    exec(f'ntwks{i}=mf_gcf[[\"jaccard_scores\"]]')\n",
    "    exec(f'ntwks{i}[\"jaccard_scores\"] = ntwks{i}[\"jaccard_scores\"].str.get({i-1})')\n",
    "\n",
    "#Concatenate jaccard and GCFs. Needs improvement cause only works with k=3\n",
    "nt_jac=pd.concat([ntwks1, ntwks2, ntwks3], axis=1)\n",
    "nt_jac.columns=[\"jaccard_scores\"+str(i) for i in range(1,k_value+1)]\n",
    "\n",
    "nt_GCFs = pd.concat([nt_GCF, nt_jac], axis=1)\n",
    "ntwks = pd.concat([ntwks.MFs, nt_GCFs], axis=1)\n",
    "ntwks = ntwks.set_index('MFs')\n",
    "\n",
    "#melt to long format each GCF with its jaccard score\n",
    "for i in range(1,k_value+1):\n",
    "    exec(f'var{i}=ntwks[[\"GCF{i}\",\"jaccard_scores{i}\"]]')\n",
    "    exec(f'var_wide{i}=pd.melt(var{i}, ignore_index=False, id_vars=[\"GCF{i}\"], value_vars=[\"jaccard_scores{i}\"])')\n",
    "    exec(f'var_wide{i}.columns=[\"GCF\",\"variable\",\"jaccard_score\"]')\n",
    "    \n",
    "ntwks_wide=pd.concat([var_wide1, var_wide2, var_wide3]) #concatenate again. Needs improvement too\n",
    "ntwks_wide.reset_index(inplace=True)\n",
    "ntwks_wide = ntwks_wide[[\"MFs\",\"GCF\",\"jaccard_score\"]]\n",
    "ntwks_wide = ntwks_wide[ntwks_wide['jaccard_score'] >= 0.7] #filter jaccard \n",
    "ntwks_wide = ntwks_wide.drop_duplicates() #remove duplicates\n",
    "ntwks_wide.to_csv(os.path.join(results_folder,'MF_GCF_network_all-NPOmix1.0-%s.txt'%(current_date)))\n",
    "\n",
    "#Filter MFs from training dataset \n",
    "ntwks_wideG2 = pd.merge(ntwks_wide, MFs_list2, how='left', on='MFs')\n",
    "ntwks_wideG2 = ntwks_wideG2[ntwks_wideG2.DefaultGroups != \"G1\"]\n",
    "ntwks_wideG2 = ntwks_wideG2.drop(columns=['count','Annotation','DefaultGroups','Type']) #remove columns\n",
    "ntwks_wideG2.to_csv(os.path.join(results_folder,'MF_GCF_network_G2-NPOmix1.0-%s.txt'%(current_date)))\n",
    "ntwks_wideG2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = subset_final_df['max_jaccard'].plot(x='metabolite_ID',figsize=(10,6))\n",
    "\n",
    "ax.set_ylim(0.65, 1.01)\n",
    "ax.set_xlabel('%s metabolites'%len(subset_final_df['max_jaccard']),size=20)\n",
    "ax.set_ylabel('Maximum jaccard similarity for k = %s'%k_value,size=20)\n",
    "\n",
    "ax\n",
    "\n",
    "#save figure\n",
    "plt.savefig(os.path.join(results_folder,'Jaccard_plot-NPOmix1.0-%s.png'%(current_date)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = pd.DataFrame(subset_final_df['parent_mass']).boxplot(figsize=[16,8],rot=90,fontsize='xx-large')\n",
    "#save figure\n",
    "plt.savefig(os.path.join(results_folder,'parentmass_boxplot-NPOmix1.0-%s.png'%(current_date))) #save plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = pd.DataFrame(subset_final_df['sum_intensity']).boxplot(figsize=[16,8],rot=90,fontsize='xx-large')\n",
    "#save figure\n",
    "plt.savefig(os.path.join(results_folder,'sum_intensity_boxplot-NPOmix1.0-%s.png'%(current_date))) #save plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [1,3,5,10,25,50,100]\n",
    "precision_scores = [100,80,90.1,85.7,60,52.17,60]\n",
    "annotation_count = [3,10,11,14,20,23,24]\n",
    "recall_list = []\n",
    "for item in annotation_count:\n",
    "    recall_list.append(round(item/max(annotation_count),2))\n",
    "    \n",
    "labels = ['k_{0}'.format(i) for i in k_list]\n",
    "\n",
    "plt.figure(figsize=(10, 10), dpi=120)\n",
    "plt.scatter(recall_list, precision_scores, s=k_list*np.repeat(25,len(k_list)))\n",
    "for label, x, y in zip(labels, recall_list, precision_scores):\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xytext=(-5,5),\n",
    "        textcoords='offset points', ha='right', va='bottom',\n",
    "        xy=(x, y))\n",
    "#plt.show()\n",
    "\n",
    "#save figure\n",
    "plt.savefig(os.path.join(results_folder,'k_list-NPOmix1.0-%s.png'%(current_date))) #save plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "run_time = \"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)\n",
    "print(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
